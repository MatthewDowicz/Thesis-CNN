{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.io import fits\n",
    "from astropy.table import Table\n",
    "from astropy.io.fits import getdata\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "import pdb\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copyfiles_fromfolder_tofolder(Root_dir,target_folder,extension):\n",
    "    RootDir1 = str(Root_dir)\n",
    "    TargetFolder = str(target_folder)\n",
    "    for root, dirs, files in os.walk((os.path.normpath(RootDir1)), topdown=False):\n",
    "        for name in files:\n",
    "            if name.endswith(str(extension)):\n",
    "                SourceFolder = os.path.join(root,name)\n",
    "                shutil.copy2(SourceFolder, TargetFolder)\n",
    "\n",
    "\n",
    "def get_filenames(path='.', extension=None, pattern=None, identifiers=None, include_path=False):\n",
    "   \n",
    "    # retrieve all filenames from the directory\n",
    "    filename_list = os.listdir(path)\n",
    "    \n",
    "    # keep all filenames with the proper extension\n",
    "    if extension is not None:\n",
    "        \n",
    "        filename_list = [filename for filename in filename_list if\n",
    "                         filename[-len(extension):] == extension]\n",
    "        \n",
    "    # keep all filenames that match the pattern\n",
    "    if pattern is not None:\n",
    "        filename_list = [filename for filename in filename_list if re.search(pattern, filename)]\n",
    "        \n",
    "    # keep all filenames that match the identifiers provided\n",
    "    if identifiers is not None:\n",
    "        storage_list = []\n",
    "        \n",
    "        try:\n",
    "            for ident in identifiers:\n",
    "                storage_list.extend([filename for filename in filename_list if str(ident) in filename])\n",
    "                \n",
    "        except TypeError:\n",
    "            print(identifiers, 'is not a list, tuple, or otherwise iterable')\n",
    "            \n",
    "        else:\n",
    "            filename_list = storage_list\n",
    "            \n",
    "    if include_path:\n",
    "        filename_list = [path + filename for filename in filename_list]\n",
    "        \n",
    "    return filename_list\n",
    "\n",
    "\n",
    "# def get_filevalues(path, filename_list): \n",
    "    \n",
    "#     # empty lists \n",
    "#     list_plate = []\n",
    "#     list_mjd = []\n",
    "#     list_fiber = []\n",
    "#     list_fluxarrays = []\n",
    "#     list_classtype = []\n",
    "#     list_noise = []\n",
    "#     list_wavelength = []\n",
    "#     list_redshift = []\n",
    "#     list_psfmag = []\n",
    "#     list_g = []\n",
    "#     list_r = []\n",
    "#     list_eboss = []\n",
    "#     list_ra = []\n",
    "#     list_dec = []\n",
    "    \n",
    "#     try: # has to do a try clause because there are some files that say hdul[1].data is \"list index out of range\"\n",
    "#          # this is causing for the code to stop and not run through the other fits files that have hdul[1].data\n",
    "    \n",
    "#         # going through all the fits files\n",
    "#         for i in range(len(filename_list)):\n",
    "        \n",
    "#             with fits.open(str(path) +str(filename_list[i])+ \"\", memmap = False ) as hdul:\n",
    "            \n",
    "#                 if hdul[1].name == \"COADD\":\n",
    "                \n",
    "#                     data_c = hdul['COADD'].data\n",
    "            \n",
    "#                 elif hdul[1].name == \"\":\n",
    "                \n",
    "#                     data_c = hdul[1].data\n",
    "           \n",
    "#                 else:\n",
    "#                     pass\n",
    "            \n",
    "            \n",
    "#                 data_s = hdul[2].data\n",
    "            \n",
    "                \n",
    "#                 flux_val = data_c.field(\"flux\")\n",
    "#                 list_fluxarrays.append(flux_val) \n",
    "                \n",
    "                \n",
    "                \n",
    "#                 plate_val = data_s.field('PLATE')\n",
    "#                 list_plate.append(plate_val)\n",
    "                \n",
    "#                 mjd_val = data_s.field('MJD')\n",
    "#                 list_mjd.append(mjd_val)\n",
    "                \n",
    "#                 fiber_val = data_s.field('FIBERID')\n",
    "#                 list_fiber.append(fiber_val)\n",
    "                \n",
    "                \n",
    "                \n",
    "#                 psfmap_val = data_s.field('PSFMAG')\n",
    "#                 list_psfmag.append(psfmap_val)\n",
    "                \n",
    "#                 g_val = data_s.field('PSFMAG')[0][1]\n",
    "#                 list_g.append(g_val)\n",
    "                \n",
    "#                 r_val = data_s.field('PSFMAG')[0][2]\n",
    "#                 list_r.append(r_val)\n",
    "                \n",
    "            \n",
    "            \n",
    "#                 classtype = data_s.field('CLASS')\n",
    "#                 list_classtype.append(classtype)\n",
    "            \n",
    "#                 noise_val = data_s.field('SN_MEDIAN_ALL')\n",
    "#                 list_noise.append(noise_val)\n",
    "            \n",
    "#                 wavelength_val = data_c.field('loglam')\n",
    "#                 list_wavelength.append(wavelength_val)\n",
    "            \n",
    "#                 redshift_val = data_s.field('Z')\n",
    "#                 list_redshift.append(redshift_val)\n",
    "                \n",
    "#                 ra_val = data_s.field('RA')\n",
    "#                 list_ra.append(ra_val)\n",
    "                \n",
    "#                 dec_val = data_s.field('DEC')\n",
    "#                 list_dec.append(dec_val)\n",
    "                \n",
    "                \n",
    "                \n",
    "#                 eboss_val = data_s.field('EBOSS_TARGET1')\n",
    "#                 list_eboss.append(eboss_val)\n",
    "            \n",
    "            \n",
    "#                 values = {'FLUX': list_fluxarrays, 'CLASS': list_classtype, 'NOISE': list_noise,\\\n",
    "#                       'WAVE': list_wavelength, 'REDSHIFT': list_redshift, 'PLATE': list_plate,\\\n",
    "#                       'MJD': list_mjd, 'FIBER': list_fiber, 'PSFMAG': list_psfmag, 'R': list_r,\\\n",
    "#                       'G': list_g,'EBOSS_TARGET1': list_eboss, 'RA': list_ra, 'DEC': list_dec}\n",
    "            \n",
    "#                 hdul.close()\n",
    "#                 del hdul[2].data\n",
    "#                 del hdul[1].data\n",
    "#                 del hdul['PRIMARY'].data\n",
    "#                 del hdul\n",
    "                \n",
    "#     except IndexError:\n",
    "#         pass\n",
    "            \n",
    "#     return values\n",
    "\n",
    "\n",
    "\n",
    "def get_filevalues(path, filename_list): \n",
    "    \n",
    "    # empty lists \n",
    "    list_plate = []\n",
    "    list_mjd = []\n",
    "    list_fiber = []\n",
    "    list_fluxarrays = []\n",
    "    list_classtype = []\n",
    "    list_noise = []\n",
    "    list_wavelength = []\n",
    "    list_redshift = []\n",
    "    list_psfmag = []\n",
    "    list_g = []\n",
    "    list_r = []\n",
    "    list_eboss = []\n",
    "    list_ra = []\n",
    "    list_dec = []\n",
    "    \n",
    "    try: # has to do a try clause because there are some files that say hdul[1].data is \"list index out of range\"\n",
    "         # this is causing for the code to stop and not run through the other fits files that have hdul[1].data\n",
    "    \n",
    "        # going through all the fits files\n",
    "        for i in range(len(filename_list)):\n",
    "        \n",
    "            with fits.open(str(path) +str(filename_list[i])+ \"\", memmap = False ) as hdul:\n",
    "            \n",
    "                if hdul[1].name == \"COADD\":\n",
    "                \n",
    "                    data_c = hdul['COADD'].data\n",
    "            \n",
    "                elif hdul[1].name == \"\":\n",
    "                \n",
    "                    data_c = hdul[1].data\n",
    "           \n",
    "                else:\n",
    "                    pass\n",
    "            \n",
    "            \n",
    "                data_s = hdul[2].data\n",
    "            \n",
    "                \n",
    "                flux_val = data_c.field(\"flux\")\n",
    "                list_fluxarrays.append(flux_val) \n",
    "                \n",
    "                \n",
    "                \n",
    "                plate_val = data_s.field('PLATE')\n",
    "                list_plate.append(plate_val)\n",
    "                \n",
    "                mjd_val = data_s.field('MJD')\n",
    "                list_mjd.append(mjd_val)\n",
    "                \n",
    "                fiber_val = data_s.field('FIBERID')\n",
    "                list_fiber.append(fiber_val)\n",
    "                \n",
    "                \n",
    "                \n",
    "                psfmap_val = data_s.field('PSFMAG')\n",
    "                list_psfmag.append(psfmap_val)\n",
    "                \n",
    "                g_val = data_s.field('PSFMAG')[0][1]\n",
    "                list_g.append(g_val)\n",
    "                \n",
    "                r_val = data_s.field('PSFMAG')[0][2]\n",
    "                list_r.append(r_val)\n",
    "                \n",
    "            \n",
    "            \n",
    "                classtype = data_s.field('CLASS')\n",
    "                list_classtype.append(classtype)\n",
    "            \n",
    "                noise_val = data_s.field('SN_MEDIAN_ALL')\n",
    "                list_noise.append(noise_val)\n",
    "            \n",
    "                wavelength_val = data_c.field('loglam')\n",
    "                list_wavelength.append(wavelength_val)\n",
    "            \n",
    "                redshift_val = data_s.field('Z')\n",
    "                list_redshift.append(redshift_val)\n",
    "                \n",
    "                ra_val = data_s.field('RA')\n",
    "                list_ra.append(ra_val)\n",
    "                \n",
    "                dec_val = data_s.field('DEC')\n",
    "                list_dec.append(dec_val)\n",
    "                \n",
    "                \n",
    "                \n",
    "                eboss_val = data_s.field('EBOSS_TARGET1')\n",
    "                list_eboss.append(eboss_val)\n",
    "            \n",
    "            \n",
    "                values = {'FLUX': list_fluxarrays, 'CLASS': list_classtype, 'NOISE': list_noise,\\\n",
    "                      'WAVE': list_wavelength, 'REDSHIFT': list_redshift, 'PLATE': list_plate,\\\n",
    "                      'MJD': list_mjd, 'FIBER': list_fiber, 'PSFMAG': list_psfmag, 'R': list_r,\\\n",
    "                      'G': list_g,'EBOSS_TARGET1': list_eboss, 'RA': list_ra, 'DEC': list_dec}\n",
    "            \n",
    "                hdul.close()\n",
    "                del hdul[2].data\n",
    "                del hdul[1].data\n",
    "                del hdul['PRIMARY'].data\n",
    "                del hdul\n",
    "                \n",
    "    except IndexError:\n",
    "        pass\n",
    "            \n",
    "            \n",
    "    return values\n",
    "\n",
    "\n",
    "    \n",
    "def save_as_pickled_object(obj, filepath):\n",
    "    \"\"\"\n",
    "    This is a defensive way to write pickle.write, allowing for very large files on all platforms\n",
    "    \"\"\"\n",
    "    max_bytes = 2**31 - 1\n",
    "    bytes_out = pickle.dumps(obj)\n",
    "    n_bytes = sys.getsizeof(bytes_out)\n",
    "    with open(filepath, 'wb') as f_out:\n",
    "        for idx in range(0, n_bytes, max_bytes):\n",
    "            f_out.write(bytes_out[idx:idx+max_bytes])\n",
    "            \n",
    "# with open('/full/path/to/file', 'wb') as f:\n",
    "#     pickle.dump(object, f)  \n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "copyfiles_fromfolder_tofolder(\"/Users/matt/Desktop/DESI_Research/DESI_ML/Data/var_quasar_data/final_var_folder/\",\\\n",
    "                              \"/Users/matt/Desktop/DESI_Research/DESI_ML/Data/var_quasar_data/final_var_fits/\",\\\n",
    "                              \".fits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "varqso_data = get_filenames(\"/Users/matt/Desktop/DESI_Research/DESI_ML/Data/var_quasar_data/final_var_fits/\", extension='.fits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "varqso_dict = get_filevalues(\"/Users/matt/Desktop/DESI_Research/DESI_ML/Data/var_quasar_data/final_var_fits/\", varqso_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_as_pickled_object(varqso_dict,'varqso_dict' )\n",
    "\n",
    "#save_as_pickled_object(varqso_dict,\"/Users/matt/Desktop/DESI_Research/DESI_ML/var_CNN/dictionaries/object_dict\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying data from redshift 2.4 < z < 3.1 where I get more data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copyfiles_fromfolder_tofolder(\"/Users/matt/Desktop/DESI_Research/DESI_ML/Data/var_quasar_data/final_var_folder_long/\",\\\n",
    "#                               \"/Users/matt/Desktop/DESI_Research/DESI_ML/Data/var_quasar_data/final_var_fits_long/\",\\\n",
    "#                               \".fits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# varqso_data = get_filenames(\"/Users/matt/Desktop/DESI_Research/DESI_ML/Data/var_quasar_data/final_var_fits_long/\",\\\n",
    "#                             extension='.fits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# varqso_dict = get_filevalues(\"/Users/matt/Desktop/DESI_Research/DESI_ML/Data/var_quasar_data/final_var_fits_long/\", varqso_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_as_pickled_object(varqso_dict,'varqso_dict_long' )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
