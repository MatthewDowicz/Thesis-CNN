{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.io import fits\n",
    "from astropy.table import Table\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "import shutil\n",
    "import pickle\n",
    "import re\n",
    "import time\n",
    "import sys\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copyfiles_fromfolder_tofolder(Root_dir,target_folder,extension):\n",
    "    RootDir1 = str(Root_dir)\n",
    "    TargetFolder = str(target_folder)\n",
    "    for root, dirs, files in os.walk((os.path.normpath(RootDir1)), topdown=False):\n",
    "        for name in files:\n",
    "            if name.endswith(str(extension)):\n",
    "                SourceFolder = os.path.join(root,name)\n",
    "                shutil.copy2(SourceFolder, TargetFolder)\n",
    "\n",
    "def get_filenames(path='.', extension=None, pattern=None, identifiers=None, include_path=False):\n",
    "   \n",
    "    # retrieve all filenames from the directory\n",
    "    filename_list = os.listdir(path)\n",
    "    \n",
    "    # keep all filenames with the proper extension\n",
    "    if extension is not None:\n",
    "        \n",
    "        filename_list = [filename for filename in filename_list if\n",
    "                         filename[-len(extension):] == extension]\n",
    "        \n",
    "    # keep all filenames that match the pattern\n",
    "    if pattern is not None:\n",
    "        filename_list = [filename for filename in filename_list if re.search(pattern, filename)]\n",
    "        \n",
    "    # keep all filenames that match the identifiers provided\n",
    "    if identifiers is not None:\n",
    "        storage_list = []\n",
    "        \n",
    "        try:\n",
    "            for ident in identifiers:\n",
    "                storage_list.extend([filename for filename in filename_list if str(ident) in filename])\n",
    "                \n",
    "        except TypeError:\n",
    "            print(identifiers, 'is not a list, tuple, or otherwise iterable')\n",
    "            \n",
    "        else:\n",
    "            filename_list = storage_list\n",
    "            \n",
    "    if include_path:\n",
    "        filename_list = [path + filename for filename in filename_list]\n",
    "        \n",
    "    return filename_list\n",
    "\n",
    "\n",
    "def get_filevalues(path, filename_list): \n",
    "    \n",
    "    # empty lists \n",
    "    list_plate = []\n",
    "    list_mjd = []\n",
    "    list_fiber = []\n",
    "    list_fluxarrays = []\n",
    "    list_classtype = []\n",
    "    list_noise = []\n",
    "    list_wavelength = []\n",
    "    list_redshift = []\n",
    "    list_psfmag = []\n",
    "    list_g = []\n",
    "    list_r = []\n",
    "    list_ra = []\n",
    "    list_dec = []\n",
    "    list_ivar = []\n",
    "    \n",
    "    # going through all the fits files\n",
    "    for i in range(len(filename_list)):\n",
    "        with fits.open(str(path) +str(filename_list[i])+ \"\", memmap = False ) as hdul:\n",
    "            \n",
    "            data_c = hdul['COADD'].data \n",
    "           \n",
    "            # the 2nd HDU is different in certain quasars\n",
    "            # this is appending all the ones with \"SPALL\" as the their 2nd HDU\n",
    "            if hdul[2].name == \"SPALL\":\n",
    "                \n",
    "                data_s = hdul['SPALL'].data\n",
    "                \n",
    "                flux_val = data_c.field(\"flux\")\n",
    "                list_fluxarrays.append(flux_val) \n",
    "                \n",
    "                \n",
    "                \n",
    "                plate_val = data_s.field('PLATE')\n",
    "                list_plate.append(plate_val)\n",
    "                \n",
    "                mjd_val = data_s.field('MJD')\n",
    "                list_mjd.append(mjd_val)\n",
    "                \n",
    "                fiber_val = data_s.field('FIBERID')\n",
    "                list_fiber.append(fiber_val)\n",
    "                \n",
    "                ivar_val = data_c.field('IVAR')\n",
    "                list_ivar.append(ivar_val)\n",
    "                \n",
    "                psfmap_val = data_s.field('PSFMAG')\n",
    "                list_psfmag.append(psfmap_val)\n",
    "                \n",
    "                g_val = data_s.field('PSFMAG')[0][1]\n",
    "                list_g.append(g_val)\n",
    "                \n",
    "                r_val = data_s.field('PSFMAG')[0][2]\n",
    "                list_r.append(r_val)\n",
    "                \n",
    "            \n",
    "            \n",
    "                classtype = data_s.field('CLASS')\n",
    "                list_classtype.append(classtype)\n",
    "            \n",
    "                noise_val = data_s.field('SN_MEDIAN_ALL')\n",
    "                list_noise.append(noise_val)\n",
    "            \n",
    "                wavelength_val = data_c.field('loglam')\n",
    "                list_wavelength.append(wavelength_val)\n",
    "            \n",
    "                redshift_val = data_s.field('Z')\n",
    "                list_redshift.append(redshift_val)\n",
    "                \n",
    "                ra_val = data_s.field('RA')\n",
    "                list_ra.append(ra_val)\n",
    "                \n",
    "                dec_val = data_s.field('DEC')\n",
    "                list_dec.append(dec_val)\n",
    "                \n",
    "                \n",
    "                del hdul['SPALL'].data\n",
    "            \n",
    "            # this is appending all the values for the quasars with \"SPECOBJ\" as their 2nd HDU\n",
    "            elif hdul[2].name == \"SPECOBJ\":\n",
    "                \n",
    "                data_s = hdul['SPECOBJ'].data\n",
    "                \n",
    "                flux_val = data_c.field(\"flux\")\n",
    "                list_fluxarrays.append(flux_val) \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                plate_val = data_s.field('PLATE')\n",
    "                list_plate.append(plate_val)\n",
    "                \n",
    "                mjd_val = data_s.field('MJD')\n",
    "                list_mjd.append(mjd_val)\n",
    "                \n",
    "                fiber_val = data_s.field('FIBERID')\n",
    "                list_fiber.append(fiber_val)\n",
    "                \n",
    "                \n",
    "                \n",
    "                psfmap_val = data_s.field('PSFMAG')\n",
    "                list_psfmag.append(psfmap_val)\n",
    "                \n",
    "                g_val = data_s.field('PSFMAG')[0][1]\n",
    "                list_g.append(g_val)\n",
    "                \n",
    "                r_val = data_s.field('PSFMAG')[0][2]\n",
    "                list_r.append(r_val)\n",
    "                \n",
    "                \n",
    "            \n",
    "                classtype = data_s.field('CLASS')\n",
    "                list_classtype.append(classtype)\n",
    "            \n",
    "                noise_val = data_s.field('SN_MEDIAN_ALL')\n",
    "                list_noise.append(noise_val)\n",
    "            \n",
    "                wavelength_val = data_c.field('loglam')\n",
    "                list_wavelength.append(wavelength_val)\n",
    "            \n",
    "                redshift_val = data_s.field('Z')\n",
    "                list_redshift.append(redshift_val)\n",
    "                \n",
    "                ra_val = data_s.field('RA')\n",
    "                list_ra.append(ra_val)\n",
    "                \n",
    "                dec_val = data_s.field('DEC')\n",
    "                list_dec.append(dec_val)\n",
    "                \n",
    "                \n",
    "                del hdul['SPECOBJ'].data\n",
    "            \n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "            \n",
    "            values = {'FLUX': list_fluxarrays, 'CLASS': list_classtype, 'NOISE': list_noise,\\\n",
    "                      'WAVE': list_wavelength, 'REDSHIFT': list_redshift, 'PLATE': list_plate,\\\n",
    "                      'MJD': list_mjd, 'FIBER': list_fiber, 'PSFMAG': list_psfmag, 'R': list_r,\\\n",
    "                      'G': list_g, 'RA': list_ra, 'DEC': list_dec, 'IVAR': list_ivar}\n",
    "            \n",
    "            hdul.close()\n",
    "            del hdul['COADD'].data\n",
    "            del hdul['PRIMARY'].data\n",
    "            del hdul\n",
    "            \n",
    "    return values    \n",
    "\n",
    "def save_data_to_disk(file_name, saved_variable):\n",
    "    \n",
    "    filename = str(file_name)\n",
    "    outfile = open(filename,'wb')\n",
    "    \n",
    "    pickle.dump(saved_variable,outfile)\n",
    "    outfile.close()\n",
    "    \n",
    "def save_as_pickled_object(obj, filepath):\n",
    "    \"\"\"\n",
    "    This is a defensive way to write pickle.write, allowing for very large files on all platforms\n",
    "    \"\"\"\n",
    "    max_bytes = 2**31 - 1\n",
    "    bytes_out = pickle.dumps(obj)\n",
    "    n_bytes = sys.getsizeof(bytes_out)\n",
    "    with open(filepath, 'wb') as f_out:\n",
    "        for idx in range(0, n_bytes, max_bytes):\n",
    "            f_out.write(bytes_out[idx:idx+max_bytes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "copyfiles_fromfolder_tofolder(\"/Users/matt/Desktop/DESI_Research/DESI_ML/Data/new_stars_folder/\",\\\n",
    "                              \"/Users/matt/Desktop/DESI_Research/DESI_ML/Data/new_stars_data/\",\".fits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stardata = get_filenames(\"/Users/matt/Desktop/DESI_Research/DESI_ML/data/new_stars_data/\", extension='.fits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "star_dict = get_filevalues(\"/Users/matt/Desktop/DESI_Research/DESI_ML/data/new_stars_data/\", stardata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_as_pickled_object(star_dict, \"/Users/matt/Desktop/DESI_Research/DESI_ML/var_CNN/Dictionaries/object_dict/star_dict\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_as_pickled_object(star_dict,'star_dict' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flux = star_dict['FLUX']\n",
    "# classification = star_dict['CLASS']\n",
    "# noise = star_dict['NOISE']\n",
    "# wave = star_dict['WAVE']\n",
    "# redshift = star_dict['REDSHIFT']\n",
    "# plate = star_dict['PLATE']\n",
    "# mjd = star_dict['MJD']\n",
    "# fiber = star_dict['FIBER']\n",
    "# psfmag = star_dict['PSFMAG']\n",
    "# g = star_dict['G']\n",
    "# r = star_dict['R']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(flux))\n",
    "# print(len(classification))\n",
    "# print(len(noise))\n",
    "# print(len(wave))\n",
    "# print(len(redshift))\n",
    "# print(len(plate))\n",
    "# print(len(mjd))\n",
    "# print(len(fiber))\n",
    "# print(len(psfmag))\n",
    "# print(len(g))\n",
    "# print(len(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(flux[:17776]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# star_dict1 = {'FLUX': flux[:17776], 'CLASS': classification[:17776], 'NOISE': noise[:17776],\\\n",
    "#                 'WAVE': wave[:17776], 'REDSHIFT': redshift[:17776], 'PLATE': plate[:17776],\\\n",
    "#                 'MJD': mjd[:17776], 'FIBER': fiber[:17776], 'PSFMAG': psfmag[:17776],\\\n",
    "#                 'G': g[:17776], 'R': r[:17776]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = 'new_star_data_dict1'\n",
    "# outfile = open(filename,'wb')\n",
    "\n",
    "# pickle.dump(star_dict1,outfile, protocol = 0)\n",
    "# outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# star_dict2 = {'FLUX': flux[17776:35551], 'CLASS': classification[17776:35551], 'NOISE': noise[17776:35551],\\\n",
    "#                 'WAVE': wave[17776:35551], 'REDSHIFT': redshift[17776:35551], 'PLATE': plate[17776:35551],\\\n",
    "#                 'MJD': mjd[17776:35551], 'FIBER': fiber[17776:35551], 'PSFMAG': psfmag[17776:35551],\\\n",
    "#                 'G': g[17776:35551], 'R': r[17776:35551]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = 'new_star_data_dict2'\n",
    "# outfile = open(filename,'wb')\n",
    "\n",
    "# pickle.dump(star_dict2,outfile, protocol = 0)\n",
    "# outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# star_dict3 = {'FLUX': flux[35551:53326], 'CLASS': classification[35551:53326], 'NOISE': noise[35551:53326],\\\n",
    "#                 'WAVE': wave[35551:53326], 'REDSHIFT': redshift[35551:53326], 'PLATE': plate[35551:53326],\\\n",
    "#                 'MJD': mjd[35551:53326], 'FIBER': fiber[35551:53326],'PSFMAG': psfmag[35551:53326],\\\n",
    "#                 'G': g[35551:53326], 'R': r[35551:53326] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = 'new_star_data_dict3'\n",
    "# outfile = open(filename,'wb')\n",
    "\n",
    "# pickle.dump(star_dict3,outfile, protocol = 0)\n",
    "# outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# star_dict4 = {'FLUX': flux[53326:], 'CLASS': classification[53326:], 'NOISE': noise[53326:],\\\n",
    "#                 'WAVE': wave[53326:], 'REDSHIFT': redshift[53326:], 'PLATE': plate[53326:],\\\n",
    "#                 'MJD': mjd[53326:], 'FIBER': fiber[53326:], 'PSFMAG': psfmag[53326:],\\\n",
    "#                 'G': g[53326:], 'R': r[53326:] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = 'new_star_data_dict4'\n",
    "# outfile = open(filename,'wb')\n",
    "\n",
    "# pickle.dump(star_dict4,outfile, protocol = 0)\n",
    "# outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
